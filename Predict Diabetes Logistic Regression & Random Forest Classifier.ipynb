{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "feab4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "0953e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that finds the best train test split size for a given model\n",
    "def best_train_size(model, df, features):\n",
    "    max_score = 0\n",
    "    best_train = 0\n",
    "    for x in np.linspace(0, 1, 21):\n",
    "        if x == 0 or x == 1:\n",
    "            continue\n",
    "        x_train, x_test, y_train, y_test = train_test_split(features, df[\"Outcome\"], train_size = x, random_state = 42)\n",
    "        model.fit(x_train, y_train)\n",
    "        score = model.score(x_test, y_test)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            best_train = x\n",
    "    return max_score, best_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "f9aacd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a fucntion that uses RFE to find the best combination of features for a given model\n",
    "def feature_elim(model, feature_columns, x_train, x_test, y_train, y_test):\n",
    "    best_score = 0\n",
    "    best_x = 0\n",
    "    for x in range(len(feature_columns)):\n",
    "        rfe = RFE(estimator = model, n_features_to_select = x + 1)\n",
    "        rfe.fit(x_train, y_train)\n",
    "        score = rfe.score(x_test, y_test)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_x = x + 1\n",
    "    final_rfe = RFE(estimator = model, n_features_to_select = best_x)\n",
    "    final_rfe.fit(x_train, y_train)\n",
    "    final_rfe.score(x_test, y_test)\n",
    "    return best_score, best_x, final_rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "7102bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in data\n",
    "df = pd.read_csv(\"/Users/cartermain/Downloads/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "44c17a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "Insulin                     0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# counting null values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6e2e8",
   "metadata": {},
   "source": [
    "This will be a logistic regression model which will require feature standardization since all values are continuous but on different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "ad720592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting features\n",
    "features = df.drop(columns = \"Outcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "d385838e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.63994726,  0.84832379,  0.14964075, ...,  0.20401277,\n",
       "         0.46849198,  1.4259954 ],\n",
       "       [-0.84488505, -1.12339636, -0.16054575, ..., -0.68442195,\n",
       "        -0.36506078, -0.19067191],\n",
       "       [ 1.23388019,  1.94372388, -0.26394125, ..., -1.10325546,\n",
       "         0.60439732, -0.10558415],\n",
       "       ...,\n",
       "       [ 0.3429808 ,  0.00330087,  0.14964075, ..., -0.73518964,\n",
       "        -0.68519336, -0.27575966],\n",
       "       [-0.84488505,  0.1597866 , -0.47073225, ..., -0.24020459,\n",
       "        -0.37110101,  1.17073215],\n",
       "       [-0.84488505, -0.8730192 ,  0.04624525, ..., -0.20212881,\n",
       "        -0.47378505, -0.87137393]])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardizing features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features)\n",
    "scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "aef86338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test splitting using random state to ensure every iteration uses the same subset\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, df[\"Outcome\"], train_size = 0.7, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "070b4936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7359307359307359\n"
     ]
    }
   ],
   "source": [
    "# training and scoring the first iteration of the logistic regression model\n",
    "lr = LogisticRegression(max_iter = 1000)\n",
    "lr.fit(x_train, y_train)\n",
    "print(lr.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "b0b57def",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
      "0     0.057768  0.035904        0.01087       0.001414  0.000984  0.109083   \n",
      "\n",
      "   DiabetesPedigreeFunction       Age  \n",
      "0                  0.374084  0.036011  \n"
     ]
    }
   ],
   "source": [
    "# checking absolute value of feature importances\n",
    "importances = pd.DataFrame(columns = features.columns, data = abs(lr.coef_))\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "7781a927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7864583333333334 0.5\n"
     ]
    }
   ],
   "source": [
    "# running through function to determine best train size \n",
    "lr_highest_score, lr_best_train = best_train_size(model, df, features_2)\n",
    "print(lr_highest_score, lr_best_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "0098a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating train test split with random state remaining as 42\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, df[\"Outcome\"], train_size = lr_best_train, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "1a77652d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7864583333333334\n"
     ]
    }
   ],
   "source": [
    "# refitting model with updated train split\n",
    "lr.fit(x_train, y_train)\n",
    "print(lr.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e506ce",
   "metadata": {},
   "source": [
    "Now we'll try to improve our model's performance via hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "87323141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7864583333333334 5 [ True  True False False False  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# running model through feature elimination function\n",
    "lr_best_score, lr_best_x, lr_support = feature_elim(lr, features.columns, x_train, x_test, y_train, y_test)\n",
    "print(lr_best_score, lr_best_x, lr_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98efce8d",
   "metadata": {},
   "source": [
    "We were able to maximize model performance while dropping 3 features which will help us conserve computational space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "34ea801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of kept feature names to create a new feature set\n",
    "lr_kept_features = []\n",
    "for x in range(len(lr_support)):\n",
    "    if lr_support[x] == True:\n",
    "        lr_kept_features.append(features.columns[x])\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "65e86dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new, refined feature set\n",
    "features_2 = df[lr_kept_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "442c29cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.63994726,  0.84832379,  0.20401277,  0.46849198,  1.4259954 ],\n",
       "       [-0.84488505, -1.12339636, -0.68442195, -0.36506078, -0.19067191],\n",
       "       [ 1.23388019,  1.94372388, -1.10325546,  0.60439732, -0.10558415],\n",
       "       ...,\n",
       "       [ 0.3429808 ,  0.00330087, -0.73518964, -0.68519336, -0.27575966],\n",
       "       [-0.84488505,  0.1597866 , -0.24020459, -0.37110101,  1.17073215],\n",
       "       [-0.84488505, -0.8730192 , -0.20212881, -0.47378505, -0.87137393]])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaling new feature set\n",
    "scaler_2 = StandardScaler()\n",
    "scaler_2.fit_transform(features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "d7cf38ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test splitting refined feature set at best train size \n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(features_2, df[\"Outcome\"], train_size = lr_best_train, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "9b89ac8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7864583333333334\n"
     ]
    }
   ],
   "source": [
    "# updating baseline model with new feature set\n",
    "lr.fit(x_train_2, y_train_2)\n",
    "print(lr.score(x_test_2, y_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cd5f1",
   "metadata": {},
   "source": [
    "Now that we have implemented feature elimination, let's take a look into hyperparameter tuning to make further improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "8905597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary of hyperparameters to test within model\n",
    "params = {\"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"], \"penalty\": [\"l1\", \"l2\"], \"C\": [100, 10, 1.0, 0.1, 0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "5f32362b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:292: UserWarning: The total space of parameters 50 is smaller than n_iter=75. Running 50 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "75 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/cartermain/opt/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.7475393         nan 0.67720437 0.7475393\n",
      " 0.75013671 0.7475393  0.68503076 0.67720437        nan        nan\n",
      " 0.7475393         nan 0.67720437 0.7475393  0.7475393  0.75273411\n",
      " 0.68503076 0.67720437        nan        nan 0.75276828        nan\n",
      " 0.67983595 0.75013671 0.75013671 0.74747095 0.68503076 0.67720437\n",
      "        nan        nan 0.71630212        nan 0.65372522 0.74757348\n",
      " 0.74757348 0.68503076 0.68766234 0.67720437        nan        nan\n",
      " 0.63544087        nan 0.64063568 0.75017088 0.75017088 0.63807245\n",
      " 0.69285714 0.65369105]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=LogisticRegression(max_iter=1000), n_iter=75,\n",
       "                   param_distributions={'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "                                        'penalty': ['l1', 'l2'],\n",
       "                                        'solver': ['newton-cg', 'lbfgs',\n",
       "                                                   'liblinear', 'sag',\n",
       "                                                   'saga']})"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using randomized search to find best hyperparameters with 75 iterations\n",
    "random_search = RandomizedSearchCV(lr, params, n_iter = 75)\n",
    "random_search.fit(x_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "2d0f1166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       solver penalty      C  Accuracy\n",
      "22  liblinear      l1   1.00  0.752768\n",
      "17  liblinear      l2  10.00  0.752734\n",
      "46      lbfgs      l2   0.01  0.750171\n",
      "45  newton-cg      l2   0.01  0.750171\n",
      "26      lbfgs      l2   1.00  0.750137\n"
     ]
    }
   ],
   "source": [
    "# concatenating and sorting to visualize performance with each set of hyperparameters in order of highest accuracy\n",
    "best_params = pd.concat([pd.DataFrame(random_search.cv_results_[\"params\"]), pd.DataFrame(random_search.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])] ,axis=1)\n",
    "print(best_params.sort_values(\"Accuracy\", ascending = False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "f96930bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78125\n"
     ]
    }
   ],
   "source": [
    "# finding best score with test values\n",
    "print(random_search.score(x_test_2, y_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1bf95",
   "metadata": {},
   "source": [
    "Our best peformer within the random search grid was lower than our base model so we can roll forward with default hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478c008",
   "metadata": {},
   "source": [
    "We were able to get relatively strong performance out of logistic regression, but let's see if we can find another classification model that will predict diabetes with even greater accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "eafad8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning each feature into 0/1 classification based on if it is below or above the feature's mean\n",
    "for x in features.columns:\n",
    "    class_features[x + \"_class\"] = df[x].apply(lambda row: 0 if row <= df[x].mean() else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "67ddff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test splitting with random state set to 42 in preparation for feature elimination and hyperparameter tuning\n",
    "x_train_4, x_test_4, y_train_4, y_test_4 = train_test_split(class_features, df[\"Outcome\"], train_size = 0.7, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "9bec34b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.658008658008658\n"
     ]
    }
   ],
   "source": [
    "# running our first iteration of the random forest classification model\n",
    "rf = RandomForestClassifier(n_estimators = 50)\n",
    "rf.fit(x_train_4, y_train_4)\n",
    "print(rf.score(x_test_4, y_test_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6fe590",
   "metadata": {},
   "source": [
    "Not great, let's see if we can improve with some feature elimination and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "e91ffd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143 3 [False  True False False  True False  True False]\n"
     ]
    }
   ],
   "source": [
    "# making use of function to find best feature set\n",
    "rf_highest_score, rf_num_features, rf_support = feature_elim(rf, class_features.columns, x_train_4, x_test_4, y_train_4, y_test_4)\n",
    "print(rf_highest_score, rf_num_features, rf_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "dc1615e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False  True False  True False]\n"
     ]
    }
   ],
   "source": [
    "print(rf_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "39acf875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending kept feature names to a list\n",
    "kept_features = []\n",
    "for x in range(len(class_features.columns)):\n",
    "    if rf_support[x] == True:\n",
    "        kept_features.append(class_features.columns[x])\n",
    "    else: \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "ea920df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating revised features list\n",
    "class_features_2 = class_features[kept_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "aea022c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test splitting revised feature set\n",
    "x_train_5, x_test_5, y_train_5, y_test_5 = train_test_split(class_features_2, df[\"Outcome\"], train_size = 0.7, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "3429b4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "# training in model with revised feature set\n",
    "rf.fit(x_train_5, y_train_5)\n",
    "print(rf.score(x_test_5, y_test_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "57c86e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7948717948717948 0.9500000000000001\n"
     ]
    }
   ],
   "source": [
    "# using function to find best train test split\n",
    "rf_max_score, rf_best_split = best_train_size(rf, df, class_features_2)\n",
    "print(rf_max_score, rf_best_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "ca3cbb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting revised feature set with best train size based on for loop results above\n",
    "best_x_train, best_x_test, best_y_train, best_y_test = train_test_split(class_features_2, df[\"Outcome\"], train_size = rf_best_split, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "6a21a8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7948717948717948\n"
     ]
    }
   ],
   "source": [
    "# training model with newly-split data\n",
    "rf.fit(best_x_train, best_y_train)\n",
    "print(rf.score(best_x_test, best_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4080fa",
   "metadata": {},
   "source": [
    "Now we just need to run some hyperparameter tuning to ensure that we are getting the most out of our model with the default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "3ff2af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary of hyperparameter set to test\n",
    "rf_params = {\"n_estimators\": range(10, 300, 20), \"criterion\": [\"gini\", \"entropy\"], \"max_depth\": range(1, len(class_features_2.columns) + 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "ebad5307",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestClassifier(n_estimators=50), n_iter=55,\n",
       "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': range(1, 4),\n",
       "                                        'n_estimators': range(10, 300, 20)})"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using random search again to find best hyperparameter set\n",
    "randomsearch_2 = RandomizedSearchCV(rf, rf_params, n_iter = 55)\n",
    "randomsearch_2.fit(best_x_train, best_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "f50d7a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    n_estimators  max_depth criterion  Accuracy\n",
      "41            10          2      gini  0.729712\n",
      "42           110          2      gini  0.724280\n",
      "34            90          2   entropy  0.722910\n",
      "2             70          3      gini  0.720132\n",
      "3             30          3      gini  0.720132\n"
     ]
    }
   ],
   "source": [
    "# concatenating and sorting to visualize performance with each set of hyperparameters in order of highest accuracy\n",
    "rf_best_params = pd.concat([pd.DataFrame(randomsearch_2.cv_results_[\"params\"]), pd.DataFrame(randomsearch_2.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])] ,axis=1)\n",
    "print(rf_best_params.sort_values(\"Accuracy\", ascending = False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "00bb1219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=2, n_estimators=10)\n",
      "0.6410256410256411\n"
     ]
    }
   ],
   "source": [
    "# finding best score and the hyperparameter set that got that score\n",
    "print(randomsearch_2.best_estimator_)\n",
    "print(randomsearch_2.score(best_x_test, best_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edade5f",
   "metadata": {},
   "source": [
    "Much like with the random search for logistic regression, we didn't find better performance outside of default values so this is about the best performance we'll see out of this and just a bit better than what we saw from our optimized logistic regression model. Moving forward, we'd use the optimized version of the random forest classifier to classify and predict probability of diabetes with ~79.5% accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
